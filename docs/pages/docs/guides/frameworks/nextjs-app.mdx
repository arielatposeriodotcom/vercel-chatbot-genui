---
title: Next.js App Router
---

import { Callout } from 'nextra-theme-docs';

# Next.js App Router

The Vercel AI SDK has been built with [Next.js App Router](https://nextjs.org/docs/app) support in mind.

Below, we will create a client component that calls a server action to generate and display chat completions.

## Client

```typescript filename="app/page.tsx" showLineNumbers
'use client';

import { useState } from 'react';
import { generate } from './actions';
import { readStreamableValue } from 'ai/rsc';

// Optional, but recommended: run on the edge runtime.
// See https://vercel.com/docs/concepts/functions/edge-functions
export const runtime = 'edge';

export default function Home() {
  const [generation, setGeneration] = useState<string | undefined>();

  return (
    <div>
      <button
        onClick={async () => {
          try {
            const { output } = await generate('Why is the sky blue?');

            for await (const content of readStreamableValue(output)) {
              setGeneration(content);
            }
          } catch (error) {
            throw error;
          }
        }}
      >
        Ask
      </button>

      <div>{generation}</div>
    </div>
  );
}
```

Note that using the Edge Runtime is optional but highly recommended due to [longer streaming timeouts on Vercel](https://vercel.com/docs/concepts/functions/edge-functions/limitations#maximum-initial-response-time), no cold starts, and lower latency.

## Server

```typescript filename="app/actions.ts" showLineNumbers
'use server';

import { OpenAI } from 'openai';
import { createStreamableValue, render } from 'ai/rsc';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export async function generate(input: string) {
  'use server';

  // Creates a wrapped, changable value that can be streamed to the client.
  const stream = createStreamableValue('');

  render({
    model: 'gpt-3.5-turbo',
    provider: openai,
    messages: [
      {
        role: 'system',
        content: 'You are a friendly assistant!',
      },
      {
        role: 'user',
        content: input,
      },
    ],
    text: ({ content, done }) => {
      if (done) {
        stream.done();
      } else {
        stream.update(content);
      }

      return null;
    },
  });

  return {
    output: stream.value,
  };
}
```

## Examples

- [next-openai](https://github.com/vercel/ai/tree/main/examples/next-openai)
- [next-replicate](https://github.com/vercel/ai/tree/main/examples/next-replicate)
- [next-huggingface](https://github.com/vercel/ai/tree/main/examples/next-huggingface)
- [next-langchain](https://github.com/vercel/ai/tree/main/examples/next-langchain)
